{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61146b25",
   "metadata": {},
   "source": [
    "# GCN Feature Matrix Creation for London LSOA Analysis\n",
    "\n",
    "This notebook creates an optimal feature matrix for Graph Convolutional Networks (GCN) using London Lower Layer Super Output Area (LSOA) data. The pipeline combines multiple data sources and applies spatial correlation-based imputation for missing values.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Data Loading & Cleaning** - Load and preprocess tabular and spatial datasets\n",
    "2. **Spatial Feature Engineering** - Calculate spatial features from geographic layers\n",
    "3. **Feature Selection** - Select optimal features for GCN analysis\n",
    "4. **Spatial Imputation** - Handle missing values using spatial correlation\n",
    "5. **Standardization** - Apply feature scaling for optimal model performance\n",
    "6. **Export Results** - Save cleaned datasets and documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8bc52",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adb7bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spatial analysis\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.ops import nearest_points\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa4c4f",
   "metadata": {},
   "source": [
    "## 2. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62cafab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_shapefiles():\n",
    "    \"\"\"Download and extract spatial data layers.\"\"\"\n",
    "    zip_paths = {\n",
    "        'lsoa': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/lsoa.zip',\n",
    "        'street': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/streetnetwork.zip',\n",
    "        'station': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/station.zip',\n",
    "        'landuse': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/landuse.zip',\n",
    "        'rail': 'https://github.com/IflyNY2PR/CASA0004/raw/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/shapefiles/railnetwork.zip'\n",
    "    }\n",
    "    \n",
    "    for name, zip_url in zip_paths.items():\n",
    "        outdir = f'./{name}'\n",
    "        zip_path = f'./{name}.zip'\n",
    "        \n",
    "        if not os.path.isdir(outdir):\n",
    "            os.makedirs(outdir, exist_ok=True)\n",
    "            print(f\"Downloading {name}...\")\n",
    "            response = requests.get(zip_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(zip_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            print(f\"Extracting {name}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                z.extractall(outdir)\n",
    "            \n",
    "            os.remove(zip_path)\n",
    "\n",
    "def find_shp(dirpath):\n",
    "    \"\"\"Find .shp file in directory.\"\"\"\n",
    "    for root, _, files in os.walk(dirpath):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.shp') and not f.startswith('._'):\n",
    "                return os.path.join(root, f)\n",
    "    raise FileNotFoundError(f\"No .shp in {dirpath}\")\n",
    "\n",
    "def load_gdf(shp_path):\n",
    "    \"\"\"Load shapefile as GeoDataFrame.\"\"\"\n",
    "    with fiona.open(shp_path) as src:\n",
    "        feats = list(src)\n",
    "        return gpd.GeoDataFrame.from_features(feats, crs=src.crs)\n",
    "\n",
    "print(\"✅ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d84c34",
   "metadata": {},
   "source": [
    "## 3. Load and Clean Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6aae3209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tabular datasets...\n",
      "  - Loading housing...\n",
      "  - Loading ptal...\n",
      "  - Loading demographics...\n",
      "  - Loading sentiment...\n",
      "Merging datasets...\n",
      "✅ Tabular data loaded and merged: (35090, 9)\n",
      "📊 PTAL Features Status:\n",
      "  MeanPTAL: 4,994 values\n",
      "  MedianPTAL: 4,994 values\n",
      "  Area_km2: 4,994 values\n"
     ]
    }
   ],
   "source": [
    "def load_tabular_data():\n",
    "    \"\"\"Load and clean all tabular datasets.\"\"\"\n",
    "    \n",
    "    # Data source URLs\n",
    "    data_urls = {\n",
    "        'housing': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/economic/housing-value.csv',\n",
    "        'ptal': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/Infrastructure/LSOA_aggregated_PTAL_stats_2023.csv',\n",
    "        'demographics': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/social/demographic/lsoa-data.csv',\n",
    "        'sentiment': 'https://raw.githubusercontent.com/IflyNY2PR/CASA0004/071702afad8b880e82c9ed33500e52ba2508e055/data-preparation/social/lsoa_sentiment_stats.csv'\n",
    "    }\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading tabular datasets...\")\n",
    "    raw_data = {}\n",
    "    for name, url in data_urls.items():\n",
    "        print(f\"  - Loading {name}...\")\n",
    "        raw_data[name] = pd.read_csv(url, encoding='latin-1', low_memory=False)\n",
    "    \n",
    "    # Clean housing data\n",
    "    hv = raw_data['housing']\n",
    "    mar_2023_cols = [c for c in hv.columns if 'Mar' in c and '2023' in c]\n",
    "    hv_price_col = mar_2023_cols[0] if mar_2023_cols else hv.columns[-1]\n",
    "    \n",
    "    hv_clean = hv[['LSOA code', hv_price_col]].copy()\n",
    "    hv_clean.columns = ['LSOA_CODE', 'AvgPrice']\n",
    "    hv_clean['AvgPrice'] = pd.to_numeric(\n",
    "        hv_clean['AvgPrice'].astype(str).str.replace(',', '').replace('', None), \n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Clean PTAL data\n",
    "    ptal = raw_data['ptal']\n",
    "    ptal_code_col = ptal.columns[0]  # Handle BOM character\n",
    "    ptal_clean = ptal[[ptal_code_col, 'mean_AI', 'MEDIAN_AI', 'Shape_Area']].rename(\n",
    "        columns={\n",
    "            ptal_code_col: 'LSOA_CODE', \n",
    "            'mean_AI': 'MeanPTAL', \n",
    "            'MEDIAN_AI': 'MedianPTAL', \n",
    "            'Shape_Area': 'Area_m2'\n",
    "        }\n",
    "    )\n",
    "    # Convert area from square meters to square kilometers\n",
    "    ptal_clean['Area_km2'] = ptal_clean['Area_m2'] / 1000000\n",
    "    # Drop the intermediate area column to keep things clean\n",
    "    ptal_clean = ptal_clean.drop('Area_m2', axis=1)\n",
    "    \n",
    "    # Clean demographics data\n",
    "    demo = raw_data['demographics']\n",
    "    demo_code_col = demo.columns[0]\n",
    "    \n",
    "    # Find population columns (we don't need area from demographics anymore since we get it from PTAL)\n",
    "    pop_cols = [c for c in demo.columns if 'Population' in c and '2023' in c]\n",
    "    if not pop_cols:\n",
    "        pop_cols = [c for c in demo.columns if 'Population' in c]\n",
    "    \n",
    "    demo_cols = [demo_code_col]\n",
    "    demo_names = ['LSOA_CODE']\n",
    "    \n",
    "    if pop_cols:\n",
    "        demo_cols.append(pop_cols[0])\n",
    "        demo_names.append('Population')\n",
    "    \n",
    "    demo_clean = demo[demo_cols].copy()\n",
    "    demo_clean.columns = demo_names\n",
    "    \n",
    "    # Clean sentiment data\n",
    "    sent = raw_data['sentiment']\n",
    "    sent_clean = sent[['LSOA', 'Avg_Sentiment_Score', 'Sentiment_Std', 'Total_Reviews']].rename(\n",
    "        columns={\n",
    "            'LSOA': 'LSOA_CODE',\n",
    "            'Avg_Sentiment_Score': 'MeanSentiment',\n",
    "            'Sentiment_Std': 'SentimentSD',\n",
    "            'Total_Reviews': 'ReviewCount'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Merge all datasets\n",
    "    print(\"Merging datasets...\")\n",
    "    df_merged = (\n",
    "        hv_clean\n",
    "        .merge(ptal_clean, on='LSOA_CODE', how='outer')\n",
    "        .merge(demo_clean, on='LSOA_CODE', how='outer')\n",
    "        .merge(sent_clean, on='LSOA_CODE', how='outer')\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Tabular data loaded and merged: {df_merged.shape}\")\n",
    "    \n",
    "    # Verify PTAL features are correctly loaded\n",
    "    ptal_features = ['MeanPTAL', 'MedianPTAL', 'Area_km2']\n",
    "    ptal_status = {}\n",
    "    for feature in ptal_features:\n",
    "        if feature in df_merged.columns:\n",
    "            non_null = df_merged[feature].notna().sum()\n",
    "            ptal_status[feature] = f\"{non_null:,} values\"\n",
    "        else:\n",
    "            ptal_status[feature] = \"MISSING\"\n",
    "    \n",
    "    print(f\"📊 PTAL Features Status:\")\n",
    "    for feature, status in ptal_status.items():\n",
    "        print(f\"  {feature}: {status}\")\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "# Load tabular data\n",
    "df_tabular = load_tabular_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301b2cf",
   "metadata": {},
   "source": [
    "## 4. Load Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "061a1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spatial datasets...\n",
      "✅ Spatial data loaded:\n",
      "  - LSOA polygons: (4719, 7)\n",
      "  - Street network: (115305, 2)\n",
      "  - Stations: (21002, 5)\n",
      "  - Land use: (30775, 5)\n",
      "  - Rail network: (11777, 5)\n",
      "  - LSOA code column: 'code'\n"
     ]
    }
   ],
   "source": [
    "# Download and extract spatial data\n",
    "download_and_extract_shapefiles()\n",
    "\n",
    "# Load spatial layers\n",
    "print(\"Loading spatial datasets...\")\n",
    "lsoa_gdf = load_gdf(find_shp('./lsoa')).to_crs('EPSG:27700')\n",
    "street_gdf = load_gdf(find_shp('./street')).to_crs('EPSG:27700')\n",
    "station_gdf = load_gdf(find_shp('./station')).to_crs('EPSG:27700')\n",
    "landuse_gdf = load_gdf(find_shp('./landuse')).to_crs('EPSG:27700')\n",
    "rail_gdf = load_gdf(find_shp('./rail')).to_crs('EPSG:27700')\n",
    "\n",
    "# Identify LSOA code column\n",
    "possible_code_cols = [c for c in lsoa_gdf.columns if 'LSOA' in c.upper() and lsoa_gdf[c].dtype == object]\n",
    "lsoa_code_col = possible_code_cols[0] if possible_code_cols else 'code'\n",
    "\n",
    "print(f\"✅ Spatial data loaded:\")\n",
    "print(f\"  - LSOA polygons: {lsoa_gdf.shape}\")\n",
    "print(f\"  - Street network: {street_gdf.shape}\")\n",
    "print(f\"  - Stations: {station_gdf.shape}\")\n",
    "print(f\"  - Land use: {landuse_gdf.shape}\")\n",
    "print(f\"  - Rail network: {rail_gdf.shape}\")\n",
    "print(f\"  - LSOA code column: '{lsoa_code_col}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa062c0",
   "metadata": {},
   "source": [
    "## 5. Filter to London LSOAs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b3823626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London LSOAs in spatial data: 4719\n",
      "London LSOAs in tabular data: 4719\n",
      "\n",
      "📊 Data Coverage Summary:\n",
      "  AvgPrice: 4,335/4,719 (91.9%)\n",
      "  MeanPTAL: 4,547/4,719 (96.4%)\n",
      "  MedianPTAL: 4,547/4,719 (96.4%)\n",
      "  Area_km2: 4,547/4,719 (96.4%)\n",
      "  Population: 4,719/4,719 (100.0%)\n",
      "  MeanSentiment: 2,678/4,719 (56.7%)\n",
      "  SentimentSD: 1,572/4,719 (33.3%)\n",
      "  ReviewCount: 2,678/4,719 (56.7%)\n",
      "\n",
      "✅ Data filtered to London LSOAs: (4719, 9)\n"
     ]
    }
   ],
   "source": [
    "# Get London LSOA codes from spatial data\n",
    "london_lsoa_codes = set(lsoa_gdf[lsoa_code_col].unique())\n",
    "print(f\"London LSOAs in spatial data: {len(london_lsoa_codes)}\")\n",
    "\n",
    "# Filter tabular data to London only\n",
    "df_london = df_tabular[df_tabular['LSOA_CODE'].isin(london_lsoa_codes)].copy()\n",
    "print(f\"London LSOAs in tabular data: {len(df_london)}\")\n",
    "\n",
    "# Data coverage summary\n",
    "print(f\"\\n📊 Data Coverage Summary:\")\n",
    "for col in df_london.columns:\n",
    "    if col != 'LSOA_CODE':\n",
    "        non_null = df_london[col].notna().sum()\n",
    "        pct = (non_null / len(df_london)) * 100\n",
    "        print(f\"  {col}: {non_null:,}/{len(df_london):,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Data filtered to London LSOAs: {df_london.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b225d18",
   "metadata": {},
   "source": [
    "## 6. Spatial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf2a5b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Calculating spatial features...\n",
      "  - Transport accessibility features...\n",
      "  - Street network features...\n",
      "  - Land use features...\n",
      "✅ Spatial features calculated: (4719, 9)\n"
     ]
    }
   ],
   "source": [
    "def calculate_spatial_features(lsoa_gdf, df_london, station_gdf, rail_gdf, street_gdf, landuse_gdf):\n",
    "    \"\"\"Calculate spatial features for each LSOA.\"\"\"\n",
    "    \n",
    "    print(\"🔧 Calculating spatial features...\")\n",
    "    \n",
    "    # Merge LSOA spatial data with tabular data\n",
    "    df_spatial = lsoa_gdf.merge(df_london, left_on=lsoa_code_col, right_on='LSOA_CODE', how='left')\n",
    "    centroids = df_spatial.geometry.centroid\n",
    "    \n",
    "    # 1. Transport accessibility features\n",
    "    print(\"  - Transport accessibility features...\")\n",
    "    station_coords = np.array([[pt.x, pt.y] for pt in station_gdf.geometry])\n",
    "    station_tree = cKDTree(station_coords)\n",
    "    centroid_coords = np.array([[pt.x, pt.y] for pt in centroids])\n",
    "    \n",
    "    # Nearest station distances\n",
    "    nearest_station_dist, _ = station_tree.query(centroid_coords)\n",
    "    \n",
    "    # Stations within 500m\n",
    "    stations_within_500m = []\n",
    "    for centroid in centroids:\n",
    "        buffer = centroid.buffer(500)\n",
    "        stations_in = station_gdf[station_gdf.geometry.within(buffer)]\n",
    "        stations_within_500m.append(len(stations_in))\n",
    "    \n",
    "    # Rail network distances\n",
    "    rail_coords = np.array([[geom.coords[0][0], geom.coords[0][1]] for geom in rail_gdf.geometry])\n",
    "    rail_tree = cKDTree(rail_coords)\n",
    "    nearest_rail_dist, _ = rail_tree.query(centroid_coords)\n",
    "    \n",
    "    # 2. Street network features\n",
    "    print(\"  - Street network features...\")\n",
    "    street_stats = []\n",
    "    for idx, lsoa in df_spatial.iterrows():\n",
    "        streets_clipped = street_gdf.clip(lsoa.geometry)\n",
    "        \n",
    "        if len(streets_clipped) > 0:\n",
    "            total_length = streets_clipped.geometry.length.sum()\n",
    "            area = lsoa.geometry.area\n",
    "            street_density = total_length / area if area > 0 else 0\n",
    "            num_segments = len(streets_clipped)\n",
    "        else:\n",
    "            total_length = 0\n",
    "            street_density = 0\n",
    "            num_segments = 0\n",
    "        \n",
    "        street_stats.append({\n",
    "            'StreetLength_m': total_length,\n",
    "            'StreetDensity_m_per_m2': street_density,\n",
    "            'StreetSegments': num_segments\n",
    "        })\n",
    "    \n",
    "    # 3. Land use features\n",
    "    print(\"  - Land use features...\")\n",
    "    landuse_cols = [c for c in landuse_gdf.columns \n",
    "                   if landuse_gdf[c].dtype == 'object' and len(landuse_gdf[c].unique()) < 50]\n",
    "    \n",
    "    if landuse_cols:\n",
    "        landuse_col = landuse_cols[0]\n",
    "        landuse_stats = []\n",
    "        \n",
    "        for idx, lsoa in df_spatial.iterrows():\n",
    "            land_int = landuse_gdf[landuse_gdf.geometry.intersects(lsoa.geometry)]\n",
    "            \n",
    "            if len(land_int) > 0:\n",
    "                diversity = land_int[landuse_col].nunique()\n",
    "                areas = []\n",
    "                for _, lu in land_int.iterrows():\n",
    "                    intersection = lu.geometry.intersection(lsoa.geometry)\n",
    "                    areas.append(intersection.area)\n",
    "                total_area = sum(areas)\n",
    "            else:\n",
    "                diversity = 0\n",
    "                total_area = 0\n",
    "            \n",
    "            landuse_stats.append({\n",
    "                'LandUse_Diversity': diversity,\n",
    "                'LandUse_Area': total_area\n",
    "            })\n",
    "    else:\n",
    "        landuse_stats = [{'LandUse_Diversity': 0, 'LandUse_Area': 0} for _ in range(len(df_spatial))]\n",
    "    \n",
    "    # Combine all spatial features\n",
    "    spatial_features = pd.DataFrame({\n",
    "        'LSOA_CODE': df_spatial['LSOA_CODE'],\n",
    "        'NearestStation_m': nearest_station_dist,\n",
    "        'StationsWithin500m': stations_within_500m,\n",
    "        'NearestRail_m': nearest_rail_dist,\n",
    "    })\n",
    "    \n",
    "    # Add street and landuse features\n",
    "    street_df = pd.DataFrame(street_stats)\n",
    "    landuse_df = pd.DataFrame(landuse_stats)\n",
    "    \n",
    "    spatial_features = pd.concat([spatial_features, street_df, landuse_df], axis=1)\n",
    "    \n",
    "    print(f\"✅ Spatial features calculated: {spatial_features.shape}\")\n",
    "    return spatial_features\n",
    "\n",
    "# Calculate spatial features\n",
    "spatial_features = calculate_spatial_features(\n",
    "    lsoa_gdf, df_london, station_gdf, rail_gdf, street_gdf, landuse_gdf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d7cf9",
   "metadata": {},
   "source": [
    "## 7. Create Combined Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c15478ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing missing Area_km2 values using spatial geometry...\n",
      "✅ Combined feature matrix created: (4719, 17)\n",
      "Features: ['LSOA_CODE', 'AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Area_km2', 'Population', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n"
     ]
    }
   ],
   "source": [
    "# Combine tabular and spatial features\n",
    "df_combined = df_london.merge(spatial_features, on='LSOA_CODE', how='left')\n",
    "\n",
    "# Area_km2 should already be available from PTAL data, but fix if needed using spatial data\n",
    "if 'Area_km2' in df_combined.columns and df_combined['Area_km2'].isnull().any():\n",
    "    print(\"Fixing missing Area_km2 values using spatial geometry...\")\n",
    "    # For any missing area values, calculate from spatial geometry\n",
    "    missing_area_mask = df_combined['Area_km2'].isnull()\n",
    "    if missing_area_mask.any():\n",
    "        area_mapping = lsoa_gdf.set_index(lsoa_code_col).geometry.area / 1000000  # Convert m² to km²\n",
    "        df_combined.loc[missing_area_mask, 'Area_km2'] = df_combined.loc[missing_area_mask, 'LSOA_CODE'].map(area_mapping)\n",
    "\n",
    "# Convert all features to numeric\n",
    "numeric_columns = [col for col in df_combined.columns if col != 'LSOA_CODE']\n",
    "for col in numeric_columns:\n",
    "    if df_combined[col].dtype == 'object':\n",
    "        df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')\n",
    "\n",
    "print(f\"✅ Combined feature matrix created: {df_combined.shape}\")\n",
    "print(f\"Features: {list(df_combined.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b366cef",
   "metadata": {},
   "source": [
    "## 8. Feature Selection for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e28ed5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features available: 16/16\n",
      "\n",
      "📊 GCN feature matrix: (4719, 17)\n",
      "Features: ['AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n",
      "\n",
      "⚠️ Features with missing values:\n",
      "  AvgPrice: 384 (8.1%)\n",
      "  MeanPTAL: 172 (3.6%)\n",
      "  MedianPTAL: 172 (3.6%)\n",
      "  MeanSentiment: 2041 (43.3%)\n",
      "  SentimentSD: 3147 (66.7%)\n",
      "  ReviewCount: 2041 (43.3%)\n"
     ]
    }
   ],
   "source": [
    "# Define optimal feature set for GCN\n",
    "selected_features = [\n",
    "    'AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Population', 'Area_km2',\n",
    "    'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m',\n",
    "    'StationsWithin500m', 'NearestRail_m', 'StreetLength_m',\n",
    "    'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area'\n",
    "]\n",
    "\n",
    "# Check feature availability\n",
    "available_features = [f for f in selected_features if f in df_combined.columns]\n",
    "missing_features = [f for f in selected_features if f not in df_combined.columns]\n",
    "\n",
    "print(f\"Selected features available: {len(available_features)}/{len(selected_features)}\")\n",
    "if missing_features:\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Create custom GCN feature matrix\n",
    "gcn_features = ['LSOA_CODE'] + available_features\n",
    "df_gcn = df_combined[gcn_features].copy()\n",
    "\n",
    "print(f\"\\n📊 GCN feature matrix: {df_gcn.shape}\")\n",
    "print(f\"Features: {available_features}\")\n",
    "\n",
    "# Check missing values\n",
    "missing_summary = df_gcn.isnull().sum()\n",
    "features_with_missing = missing_summary[missing_summary > 0]\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    print(f\"\\n⚠️ Features with missing values:\")\n",
    "    for feature, count in features_with_missing.items():\n",
    "        pct = (count / len(df_gcn)) * 100\n",
    "        print(f\"  {feature}: {count} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n✅ No missing values in feature matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b4c1f",
   "metadata": {},
   "source": [
    "## 9. Spatial Correlation-Based Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b5840bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying spatial imputation to 6 features...\n",
      "  Processing AvgPrice...\n",
      "    Imputed 384 values\n",
      "  Processing MeanPTAL...\n",
      "    Imputed 172 values\n",
      "  Processing MedianPTAL...\n",
      "    Imputed 172 values\n",
      "  Processing MeanSentiment...\n",
      "    Imputed 2041 values\n",
      "  Processing SentimentSD...\n",
      "    Imputed 3147 values\n",
      "  Processing ReviewCount...\n",
      "    Imputed 2041 values\n",
      "✅ Spatial imputation completed. Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "def spatial_imputation(df_gcn, lsoa_gdf, lsoa_code_col):\n",
    "    \"\"\"Apply spatial correlation-based imputation for missing values.\"\"\"\n",
    "    \n",
    "    missing_summary = df_gcn.isnull().sum()\n",
    "    features_with_missing = missing_summary[missing_summary > 0]\n",
    "    \n",
    "    if len(features_with_missing) == 0:\n",
    "        print(\"✅ No missing values found - no imputation needed\")\n",
    "        return df_gcn.copy()\n",
    "    \n",
    "    print(f\"🔧 Applying spatial imputation to {len(features_with_missing)} features...\")\n",
    "    \n",
    "    df_imputed = df_gcn.copy()\n",
    "    \n",
    "    # Merge with spatial data for spatial operations\n",
    "    spatial_data = lsoa_gdf[[lsoa_code_col, 'geometry']].copy()\n",
    "    df_with_geometry = pd.merge(\n",
    "        df_imputed, spatial_data,\n",
    "        left_on='LSOA_CODE', right_on=lsoa_code_col, how='left'\n",
    "    )\n",
    "    df_with_geometry = gpd.GeoDataFrame(df_with_geometry, geometry='geometry')\n",
    "    \n",
    "    # Apply spatial imputation for each feature with missing values\n",
    "    for feature in features_with_missing.index:\n",
    "        if feature == 'LSOA_CODE':\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing {feature}...\")\n",
    "        missing_mask = df_with_geometry[feature].isnull()\n",
    "        missing_indices = df_with_geometry[missing_mask].index\n",
    "        \n",
    "        imputed_values = []\n",
    "        \n",
    "        for idx in missing_indices:\n",
    "            missing_geometry = df_with_geometry.loc[idx, 'geometry']\n",
    "            \n",
    "            if missing_geometry is None or pd.isna(missing_geometry):\n",
    "                # Use median if no geometry\n",
    "                fill_value = df_with_geometry[feature].median()\n",
    "            else:\n",
    "                # Try different distance thresholds\n",
    "                fill_value = None\n",
    "                for distance in [500, 1000, 2000, 5000, 10000]:\n",
    "                    buffer = missing_geometry.buffer(distance)\n",
    "                    intersecting = df_with_geometry[df_with_geometry.geometry.intersects(buffer)]\n",
    "                    neighbor_values = intersecting[feature].dropna()\n",
    "                    neighbor_values = neighbor_values[neighbor_values.index != idx]\n",
    "                    \n",
    "                    if len(neighbor_values) >= 3:\n",
    "                        # Distance-weighted average\n",
    "                        weights = []\n",
    "                        values = []\n",
    "                        \n",
    "                        for neighbor_idx in neighbor_values.index:\n",
    "                            neighbor_geom = df_with_geometry.loc[neighbor_idx, 'geometry']\n",
    "                            if neighbor_geom is not None:\n",
    "                                dist = missing_geometry.distance(neighbor_geom)\n",
    "                                if dist > 0:\n",
    "                                    weight = 1 / (dist + 100)\n",
    "                                    weights.append(weight)\n",
    "                                    values.append(neighbor_values.loc[neighbor_idx])\n",
    "                        \n",
    "                        if len(values) > 0:\n",
    "                            weights = np.array(weights)\n",
    "                            values = np.array(values)\n",
    "                            fill_value = np.average(values, weights=weights)\n",
    "                            break\n",
    "                    elif len(neighbor_values) >= 1:\n",
    "                        fill_value = neighbor_values.mean()\n",
    "                        break\n",
    "                \n",
    "                # Fallback to median if no spatial neighbors found\n",
    "                if fill_value is None or np.isnan(fill_value):\n",
    "                    fill_value = df_with_geometry[feature].median()\n",
    "            \n",
    "            imputed_values.append(fill_value)\n",
    "        \n",
    "        # Apply imputed values\n",
    "        df_imputed.loc[df_imputed[feature].isnull(), feature] = imputed_values\n",
    "        print(f\"    Imputed {len(imputed_values)} values\")\n",
    "    \n",
    "    # Verify no missing values remain\n",
    "    remaining_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"✅ Spatial imputation completed. Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# Apply spatial imputation\n",
    "df_gcn_imputed = spatial_imputation(df_gcn, lsoa_gdf, lsoa_code_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8852c",
   "metadata": {},
   "source": [
    "## 10. Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3da98950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features standardized: (4719, 17)\n",
      "Feature means (should be ~0): 0.000000\n",
      "Feature stds (should be ~1): 1.000 to 1.000\n",
      "\n",
      "📊 Sample of standardized GCN feature matrix:\n",
      "   LSOA_CODE  AvgPrice  MeanPTAL  MedianPTAL  Population  Area_km2  \\\n",
      "0  E01000001  0.553797  5.498077    5.524460    0.435383 -0.407064   \n",
      "1  E01000002  0.586108  6.435168    6.640578   -0.090485 -0.153782   \n",
      "2  E01000003 -0.215199  3.588845    3.595403    0.254346 -0.589046   \n",
      "3  E01000005 -0.073284  6.176879    6.443612   -1.827576 -0.253605   \n",
      "4  E01000006 -0.988072  0.979416    1.303275    0.422452 -0.364218   \n",
      "\n",
      "   MeanSentiment  SentimentSD  ReviewCount  NearestStation_m  \\\n",
      "0      -1.238606     1.682900    -0.005206         -1.240113   \n",
      "1      -0.479543     0.370189     0.422149          0.217765   \n",
      "2      -2.099400     0.226084    -0.153997         -0.467001   \n",
      "3       0.702093    -0.561413    -0.048595         -1.107029   \n",
      "4      -0.400212    -0.189228    -0.024191          1.988550   \n",
      "\n",
      "   StationsWithin500m  NearestRail_m  StreetLength_m  StreetDensity_m_per_m2  \\\n",
      "0            2.044513      -0.798986       -0.333851                0.966407   \n",
      "1            2.184298      -0.960413        0.541895                0.556147   \n",
      "2            1.904729      -0.868722       -1.482122               -0.408466   \n",
      "3            2.883220      -1.195270        0.686240                1.405753   \n",
      "4           -0.611390      -0.941793       -0.157568                0.913186   \n",
      "\n",
      "   StreetSegments  LandUse_Diversity  LandUse_Area  \n",
      "0        0.438961           0.300171     -0.554583  \n",
      "1        1.129350           0.871112     -0.415063  \n",
      "2       -1.402077          -0.270771     -0.581698  \n",
      "3        1.992336           0.300171     -0.304768  \n",
      "4       -0.539091          -1.412653     -0.331160  \n"
     ]
    }
   ],
   "source": [
    "# Standardize features (mean=0, std=1)\n",
    "feature_cols = [col for col in df_gcn_imputed.columns if col != 'LSOA_CODE']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standardization\n",
    "features_scaled = scaler.fit_transform(df_gcn_imputed[feature_cols])\n",
    "\n",
    "# Create standardized dataframe\n",
    "df_gcn_scaled = pd.DataFrame(\n",
    "    features_scaled, \n",
    "    columns=feature_cols,\n",
    "    index=df_gcn_imputed.index\n",
    ")\n",
    "df_gcn_scaled['LSOA_CODE'] = df_gcn_imputed['LSOA_CODE'].values\n",
    "\n",
    "# Reorder columns\n",
    "df_gcn_scaled = df_gcn_scaled[['LSOA_CODE'] + feature_cols]\n",
    "\n",
    "print(f\"✅ Features standardized: {df_gcn_scaled.shape}\")\n",
    "print(f\"Feature means (should be ~0): {df_gcn_scaled[feature_cols].mean().abs().max():.6f}\")\n",
    "print(f\"Feature stds (should be ~1): {df_gcn_scaled[feature_cols].std().min():.3f} to {df_gcn_scaled[feature_cols].std().max():.3f}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\n📊 Sample of standardized GCN feature matrix:\")\n",
    "print(df_gcn_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55005b4f",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb7b2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving GCN feature matrices...\n",
      "✅ Saved imputed matrix: gcn_feature_matrix_spatial_imputed.csv\n",
      "✅ Saved scaled matrix: gcn_feature_matrix_spatial_imputed_scaled.csv (RECOMMENDED)\n",
      "✅ Saved matrix with geometry: gcn_feature_matrix_with_geometry.csv\n",
      "✅ Saved documentation: gcn_feature_matrix_documentation.csv\n",
      "✅ Saved summary statistics: gcn_feature_matrix_summary_stats.csv\n",
      "\n",
      "🎉 GCN FEATURE MATRIX PIPELINE COMPLETED!\n",
      "============================================================\n",
      "📁 OUTPUT FILES:\n",
      "  gcn_feature_matrix_spatial_imputed.csv (0.99 MB)\n",
      "  gcn_feature_matrix_spatial_imputed_scaled.csv (1.48 MB)\n",
      "  gcn_feature_matrix_with_geometry.csv (58.16 MB)\n",
      "  gcn_feature_matrix_documentation.csv (0.00 MB)\n",
      "  gcn_feature_matrix_summary_stats.csv (0.00 MB)\n",
      "\n",
      "🎯 RECOMMENDED FOR GCN: gcn_feature_matrix_spatial_imputed_scaled.csv\n",
      "  • Shape: (4719, 17)\n",
      "  • Features: 16 optimized variables\n",
      "  • Missing values: 0 (spatially imputed)\n",
      "  • Standardized: Yes (mean≈0, std≈1)\n",
      "  • Ready for: Graph Convolutional Network training\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define output files\n",
    "output_files = {\n",
    "    'imputed': 'gcn_feature_matrix_spatial_imputed.csv',\n",
    "    'scaled': 'gcn_feature_matrix_spatial_imputed_scaled.csv',\n",
    "    'with_geometry': 'gcn_feature_matrix_with_geometry.csv',\n",
    "    'documentation': 'gcn_feature_matrix_documentation.csv',\n",
    "    'summary_stats': 'gcn_feature_matrix_summary_stats.csv'\n",
    "}\n",
    "\n",
    "print(\"💾 Saving GCN feature matrices...\")\n",
    "\n",
    "# 1. Save imputed (unscaled) matrix\n",
    "df_gcn_imputed.to_csv(output_files['imputed'], index=False)\n",
    "print(f\"✅ Saved imputed matrix: {output_files['imputed']}\")\n",
    "\n",
    "# 2. Save scaled matrix (RECOMMENDED)\n",
    "df_gcn_scaled.to_csv(output_files['scaled'], index=False)\n",
    "print(f\"✅ Saved scaled matrix: {output_files['scaled']} (RECOMMENDED)\")\n",
    "\n",
    "# 3. Save matrix with geometry\n",
    "df_with_geometry = pd.merge(\n",
    "    df_gcn_scaled, lsoa_gdf[[lsoa_code_col, 'geometry']],\n",
    "    left_on='LSOA_CODE', right_on=lsoa_code_col, how='left'\n",
    ").drop(lsoa_code_col, axis=1)\n",
    "\n",
    "df_with_geometry['geometry_wkt'] = df_with_geometry['geometry'].apply(\n",
    "    lambda x: x.wkt if x is not None else None\n",
    ")\n",
    "df_geometry_export = df_with_geometry.drop('geometry', axis=1)\n",
    "df_geometry_export.to_csv(output_files['with_geometry'], index=False)\n",
    "print(f\"✅ Saved matrix with geometry: {output_files['with_geometry']}\")\n",
    "\n",
    "# 4. Create and save documentation\n",
    "feature_docs = []\n",
    "feature_descriptions = {\n",
    "    'AvgPrice': 'Average housing price (March 2023, £)',\n",
    "    'MeanPTAL': 'Mean Public Transport Accessibility Index',\n",
    "    'MedianPTAL': 'Median Public Transport Accessibility Index', \n",
    "    'Population': 'Population count in LSOA',\n",
    "    'Area_km2': 'LSOA area (square kilometers, from PTAL dataset)',\n",
    "    'MeanSentiment': 'Average sentiment score (-1 to 1)',\n",
    "    'SentimentSD': 'Standard deviation of sentiment scores',\n",
    "    'ReviewCount': 'Number of reviews/posts analyzed',\n",
    "    'NearestStation_m': 'Distance to nearest station (meters)',\n",
    "    'StationsWithin500m': 'Number of stations within 500m',\n",
    "    'NearestRail_m': 'Distance to nearest rail line (meters)',\n",
    "    'StreetLength_m': 'Total street length (meters)',\n",
    "    'StreetDensity_m_per_m2': 'Street density (m/m²)',\n",
    "    'StreetSegments': 'Number of street segments',\n",
    "    'LandUse_Diversity': 'Number of land use types',\n",
    "    'LandUse_Area': 'Total land use area (m²)'\n",
    "}\n",
    "\n",
    "for feature in feature_cols:\n",
    "    feature_docs.append({\n",
    "        'Feature': feature,\n",
    "        'Description': feature_descriptions.get(feature, 'No description'),\n",
    "        'Data_Type': 'float64 (standardized)',\n",
    "        'Missing_Values': 'Spatially imputed',\n",
    "        'Standardized': 'Yes (mean≈0, std≈1)'\n",
    "    })\n",
    "\n",
    "pd.DataFrame(feature_docs).to_csv(output_files['documentation'], index=False)\n",
    "print(f\"✅ Saved documentation: {output_files['documentation']}\")\n",
    "\n",
    "# 5. Save summary statistics\n",
    "df_gcn_scaled[feature_cols].describe().to_csv(output_files['summary_stats'])\n",
    "print(f\"✅ Saved summary statistics: {output_files['summary_stats']}\")\n",
    "\n",
    "print(f\"\\n🎉 GCN FEATURE MATRIX PIPELINE COMPLETED!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"📁 OUTPUT FILES:\")\n",
    "for purpose, filename in output_files.items():\n",
    "    size = os.path.getsize(filename) / (1024*1024)\n",
    "    print(f\"  {filename} ({size:.2f} MB)\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDED FOR GCN: {output_files['scaled']}\")\n",
    "print(f\"  • Shape: {df_gcn_scaled.shape}\")\n",
    "print(f\"  • Features: {len(feature_cols)} optimized variables\")\n",
    "print(f\"  • Missing values: 0 (spatially imputed)\")\n",
    "print(f\"  • Standardized: Yes (mean≈0, std≈1)\")\n",
    "print(f\"  • Ready for: Graph Convolutional Network training\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c667145",
   "metadata": {},
   "source": [
    "## 12. Final Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b76741f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Final verification of GCN feature matrix:\n",
      "  📏 Shape: (4719, 17)\n",
      "  🏷️  Features: ['LSOA_CODE', 'AvgPrice', 'MeanPTAL', 'MedianPTAL', 'Population', 'Area_km2', 'MeanSentiment', 'SentimentSD', 'ReviewCount', 'NearestStation_m', 'StationsWithin500m', 'NearestRail_m', 'StreetLength_m', 'StreetDensity_m_per_m2', 'StreetSegments', 'LandUse_Diversity', 'LandUse_Area']\n",
      "  📊 Missing values: 0\n",
      "  📊 Mean range: -0.000000 to 0.000000 (should be ≈ 0)\n",
      "  📊 Std range: 1.000106 to 1.000106 (should be ≈ 1)\n",
      "\n",
      "✅ VERIFICATION PASSED - GCN feature matrix is ready for training!\n",
      "\n",
      "💡 UNDERSTANDING NEGATIVE VALUES:\n",
      "  Negative values are NORMAL and expected after standardization.\n",
      "  They represent areas with below-average characteristics:\n",
      "  • AvgPrice < 0: Below-average housing prices\n",
      "  • Population < 0: Below-average population density\n",
      "  • MeanSentiment < 0: Below-average sentiment scores\n",
      "  • etc.\n",
      "\n",
      "  This is the correct format for GCN training! 🎯\n"
     ]
    }
   ],
   "source": [
    "# Load and verify the recommended file\n",
    "gcn_final = pd.read_csv(output_files['scaled'])\n",
    "\n",
    "print(\"🔍 Final verification of GCN feature matrix:\")\n",
    "print(f\"  📏 Shape: {gcn_final.shape}\")\n",
    "print(f\"  🏷️  Features: {list(gcn_final.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_check = gcn_final.isnull().sum().sum()\n",
    "print(f\"  📊 Missing values: {missing_check}\")\n",
    "\n",
    "# Check standardization\n",
    "numeric_features = [col for col in gcn_final.columns if col != 'LSOA_CODE']\n",
    "means = gcn_final[numeric_features].mean()\n",
    "stds = gcn_final[numeric_features].std()\n",
    "\n",
    "print(f\"  📊 Mean range: {means.min():.6f} to {means.max():.6f} (should be ≈ 0)\")\n",
    "print(f\"  📊 Std range: {stds.min():.6f} to {stds.max():.6f} (should be ≈ 1)\")\n",
    "\n",
    "print(f\"\\n✅ VERIFICATION PASSED - GCN feature matrix is ready for training!\")\n",
    "\n",
    "# Display interpretation of negative values\n",
    "print(f\"\\n💡 UNDERSTANDING NEGATIVE VALUES:\")\n",
    "print(f\"  Negative values are NORMAL and expected after standardization.\")\n",
    "print(f\"  They represent areas with below-average characteristics:\")\n",
    "print(f\"  • AvgPrice < 0: Below-average housing prices\")\n",
    "print(f\"  • Population < 0: Below-average population density\")\n",
    "print(f\"  • MeanSentiment < 0: Below-average sentiment scores\")\n",
    "print(f\"  • etc.\")\n",
    "print(f\"\\n  This is the correct format for GCN training! 🎯\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
